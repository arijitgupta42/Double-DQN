{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-dSl6ZTT6UCF"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as opt\n",
    "import numpy as np\n",
    "import gym \n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from collections import deque\n",
    "import random\n",
    "import os\n",
    "import time\n",
    "os.environ.setdefault('PATH', '')\n",
    "from collections import deque\n",
    "from gym import spaces\n",
    "import cv2\n",
    "cv2.ocl.setUseOpenCL(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D-Jb0tYP6aXn"
   },
   "outputs": [],
   "source": [
    "# helpers\n",
    "def init_weights(m):\n",
    "\tif isinstance(m,nn.Linear):\n",
    "\t\ttorch.nn.init.normal_(m.weight,0.,0.1)\n",
    "\n",
    "class ReplayBuffer:\n",
    "\tdef __init__(self,size):\n",
    "\t\tself.size = size\n",
    "\t\tself.memory = deque([],maxlen=size)\n",
    "\n",
    "\tdef push(self, x):\n",
    "\t\tself.memory.append(x)\n",
    "\t\n",
    "\tdef sample(self, batch_size):\n",
    "\t\tbatch = random.sample(self.memory,batch_size)\n",
    "\t\tstate, action, reward, next_state, done = map(np.stack, zip(*batch))\n",
    "\t\treturn state, action, reward, next_state, done\n",
    "\n",
    "\tdef get_len(self):\n",
    "\t\treturn len(self.memory)\n",
    "\n",
    "class TimeLimit(gym.Wrapper):\n",
    "\tdef __init__(self, env, max_episode_steps=None):\n",
    "\t\tsuper(TimeLimit, self).__init__(env)\n",
    "\t\tself._max_episode_steps = max_episode_steps\n",
    "\t\tself._elapsed_steps = 0\n",
    "\n",
    "\tdef step(self, ac):\n",
    "\t\tobservation, reward, done, info = self.env.step(ac)\n",
    "\t\tself._elapsed_steps += 1\n",
    "\t\tif self._elapsed_steps >= self._max_episode_steps:\n",
    "\t\t\tdone = True\n",
    "\t\t\tinfo['TimeLimit.truncated'] = True\n",
    "\t\treturn observation, reward, done, info\n",
    "\n",
    "\tdef reset(self, **kwargs):\n",
    "\t\tself._elapsed_steps = 0\n",
    "\t\treturn self.env.reset(**kwargs)\n",
    "\n",
    "class ClipActionsWrapper(gym.Wrapper):\n",
    "\tdef step(self, action):\n",
    "\t\timport numpy as np\n",
    "\t\taction = np.nan_to_num(action)\n",
    "\t\taction = np.clip(action, self.action_space.low, self.action_space.high)\n",
    "\t\treturn self.env.step(action)\n",
    "\n",
    "\tdef reset(self, **kwargs):\n",
    "\t\treturn self.env.reset(**kwargs)\n",
    "\n",
    "class NoopResetEnv(gym.Wrapper):\n",
    "\tdef __init__(self, env, noop_max=30):\n",
    "\t\t\"\"\"Sample initial states by taking random number of no-ops on reset.\n",
    "\t\tNo-op is assumed to be action 0.\n",
    "\t\t\"\"\"\n",
    "\t\tgym.Wrapper.__init__(self, env)\n",
    "\t\tself.noop_max = noop_max\n",
    "\t\tself.override_num_noops = None\n",
    "\t\tself.noop_action = 0\n",
    "\t\tassert env.unwrapped.get_action_meanings()[0] == 'NOOP'\n",
    "\n",
    "\tdef reset(self, **kwargs):\n",
    "\t\t\"\"\" Do no-op action for a number of steps in [1, noop_max].\"\"\"\n",
    "\t\tself.env.reset(**kwargs)\n",
    "\t\tif self.override_num_noops is not None:\n",
    "\t\t\tnoops = self.override_num_noops\n",
    "\t\telse:\n",
    "\t\t\tnoops = self.unwrapped.np_random.randint(1, self.noop_max + 1) #pylint: disable=E1101\n",
    "\t\tassert noops > 0\n",
    "\t\tobs = None\n",
    "\t\tfor _ in range(noops):\n",
    "\t\t\tobs, _, done, _ = self.env.step(self.noop_action)\n",
    "\t\t\tif done:\n",
    "\t\t\t\tobs = self.env.reset(**kwargs)\n",
    "\t\treturn obs\n",
    "\n",
    "\tdef step(self, ac):\n",
    "\t\treturn self.env.step(ac)\n",
    "\n",
    "class FireResetEnv(gym.Wrapper):\n",
    "\tdef __init__(self, env):\n",
    "\t\t\"\"\"Take action on reset for environments that are fixed until firing.\"\"\"\n",
    "\t\tgym.Wrapper.__init__(self, env)\n",
    "\t\tassert env.unwrapped.get_action_meanings()[1] == 'FIRE'\n",
    "\t\tassert len(env.unwrapped.get_action_meanings()) >= 3\n",
    "\n",
    "\tdef reset(self, **kwargs):\n",
    "\t\tself.env.reset(**kwargs)\n",
    "\t\tobs, _, done, _ = self.env.step(1)\n",
    "\t\tif done:\n",
    "\t\t\tself.env.reset(**kwargs)\n",
    "\t\tobs, _, done, _ = self.env.step(2)\n",
    "\t\tif done:\n",
    "\t\t\tself.env.reset(**kwargs)\n",
    "\t\treturn obs\n",
    "\n",
    "\tdef step(self, ac):\n",
    "\t\treturn self.env.step(ac)\n",
    "\n",
    "class EpisodicLifeEnv(gym.Wrapper):\n",
    "\tdef __init__(self, env):\n",
    "\t\t\"\"\"Make end-of-life == end-of-episode, but only reset on true game over.\n",
    "\t\tDone by DeepMind for the DQN and co. since it helps value estimation.\n",
    "\t\t\"\"\"\n",
    "\t\tgym.Wrapper.__init__(self, env)\n",
    "\t\tself.lives = 0\n",
    "\t\tself.was_real_done  = True\n",
    "\n",
    "\tdef step(self, action):\n",
    "\t\tobs, reward, done, info = self.env.step(action)\n",
    "\t\tself.was_real_done = done\n",
    "\t\t# check current lives, make loss of life terminal,\n",
    "\t\t# then update lives to handle bonus lives\n",
    "\t\tlives = self.env.unwrapped.ale.lives()\n",
    "\t\tif lives < self.lives and lives > 0:\n",
    "\t\t\t# for Qbert sometimes we stay in lives == 0 condition for a few frames\n",
    "\t\t\t# so it's important to keep lives > 0, so that we only reset once\n",
    "\t\t\t# the environment advertises done.\n",
    "\t\t\tdone = True\n",
    "\t\tself.lives = lives\n",
    "\t\treturn obs, reward, done, info\n",
    "\n",
    "\tdef reset(self, **kwargs):\n",
    "\t\t\"\"\"Reset only when lives are exhausted.\n",
    "\t\tThis way all states are still reachable even though lives are episodic,\n",
    "\t\tand the learner need not know about any of this behind-the-scenes.\n",
    "\t\t\"\"\"\n",
    "\t\tif self.was_real_done:\n",
    "\t\t\tobs = self.env.reset(**kwargs)\n",
    "\t\telse:\n",
    "\t\t\t# no-op step to advance from terminal/lost life state\n",
    "\t\t\tobs, _, _, _ = self.env.step(0)\n",
    "\t\tself.lives = self.env.unwrapped.ale.lives()\n",
    "\t\treturn obs\n",
    "\n",
    "class MaxAndSkipEnv(gym.Wrapper):\n",
    "\tdef __init__(self, env, skip=4):\n",
    "\t\t\"\"\"Return only every `skip`-th frame\"\"\"\n",
    "\t\tgym.Wrapper.__init__(self, env)\n",
    "\t\t# most recent raw observations (for max pooling across time steps)\n",
    "\t\tself._obs_buffer = np.zeros((2,)+env.observation_space.shape, dtype=np.uint8)\n",
    "\t\tself._skip       = skip\n",
    "\n",
    "\tdef step(self, action):\n",
    "\t\t\"\"\"Repeat action, sum reward, and max over last observations.\"\"\"\n",
    "\t\ttotal_reward = 0.0\n",
    "\t\tdone = None\n",
    "\t\tfor i in range(self._skip):\n",
    "\t\t\tobs, reward, done, info = self.env.step(action)\n",
    "\t\t\tif i == self._skip - 2: self._obs_buffer[0] = obs\n",
    "\t\t\tif i == self._skip - 1: self._obs_buffer[1] = obs\n",
    "\t\t\ttotal_reward += reward\n",
    "\t\t\tif done:\n",
    "\t\t\t\tbreak\n",
    "\t\t# Note that the observation on the done=True frame\n",
    "\t\t# doesn't matter\n",
    "\t\tmax_frame = self._obs_buffer.max(axis=0)\n",
    "\n",
    "\t\treturn max_frame, total_reward, done, info\n",
    "\n",
    "\tdef reset(self, **kwargs):\n",
    "\t\treturn self.env.reset(**kwargs)\n",
    "\n",
    "class ClipRewardEnv(gym.RewardWrapper):\n",
    "\tdef __init__(self, env):\n",
    "\t\tgym.RewardWrapper.__init__(self, env)\n",
    "\n",
    "\tdef reward(self, reward):\n",
    "\t\t\"\"\"Bin reward to {+1, 0, -1} by its sign.\"\"\"\n",
    "\t\treturn np.sign(reward)\n",
    "\n",
    "class WarpFrame(gym.ObservationWrapper):\n",
    "\tdef __init__(self, env, width=84, height=84, grayscale=True, dict_space_key=None):\n",
    "\t\t\"\"\"\n",
    "\t\tWarp frames to 84x84 as done in the Nature paper and later work.\n",
    "\t\tIf the environment uses dictionary observations, `dict_space_key` can be specified which indicates which\n",
    "\t\tobservation should be warped.\n",
    "\t\t\"\"\"\n",
    "\t\tsuper().__init__(env)\n",
    "\t\tself._width = width\n",
    "\t\tself._height = height\n",
    "\t\tself._grayscale = grayscale\n",
    "\t\tself._key = dict_space_key\n",
    "\t\tif self._grayscale:\n",
    "\t\t\tnum_colors = 1\n",
    "\t\telse:\n",
    "\t\t\tnum_colors = 3\n",
    "\n",
    "\t\tnew_space = gym.spaces.Box(\n",
    "\t\t\tlow=0,\n",
    "\t\t\thigh=255,\n",
    "\t\t\tshape=(self._height, self._width, num_colors),\n",
    "\t\t\tdtype=np.uint8,\n",
    "\t\t)\n",
    "\t\tif self._key is None:\n",
    "\t\t\toriginal_space = self.observation_space\n",
    "\t\t\tself.observation_space = new_space\n",
    "\t\telse:\n",
    "\t\t\toriginal_space = self.observation_space.spaces[self._key]\n",
    "\t\t\tself.observation_space.spaces[self._key] = new_space\n",
    "\t\tassert original_space.dtype == np.uint8 and len(original_space.shape) == 3\n",
    "\n",
    "\tdef observation(self, obs):\n",
    "\t\tif self._key is None:\n",
    "\t\t\tframe = obs\n",
    "\t\telse:\n",
    "\t\t\tframe = obs[self._key]\n",
    "\n",
    "\t\tif self._grayscale:\n",
    "\t\t\tframe = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
    "\t\tframe = cv2.resize(\n",
    "\t\t\tframe, (self._width, self._height), interpolation=cv2.INTER_AREA\n",
    "\t\t)\n",
    "\t\tif self._grayscale:\n",
    "\t\t\tframe = np.expand_dims(frame, -1)\n",
    "\n",
    "\t\tif self._key is None:\n",
    "\t\t\tobs = frame\n",
    "\t\telse:\n",
    "\t\t\tobs = obs.copy()\n",
    "\t\t\tobs[self._key] = frame\n",
    "\t\treturn obs\n",
    "\n",
    "class FrameStack(gym.Wrapper):\n",
    "\tdef __init__(self, env, k):\n",
    "\t\t\"\"\"Stack k last frames.\n",
    "\t\tReturns lazy array, which is much more memory efficient.\n",
    "\t\tSee Also\n",
    "\t\t--------\n",
    "\t\tbaselines.common.atari_wrappers.LazyFrames\n",
    "\t\t\"\"\"\n",
    "\t\tgym.Wrapper.__init__(self, env)\n",
    "\t\tself.k = k\n",
    "\t\tself.frames = deque([], maxlen=k)\n",
    "\t\tshp = env.observation_space.shape\n",
    "\t\tself.observation_space = spaces.Box(low=0, high=255, shape=(shp[:-1] + (shp[-1] * k,)), dtype=env.observation_space.dtype)\n",
    "\n",
    "\tdef reset(self):\n",
    "\t\tob = self.env.reset()\n",
    "\t\tfor _ in range(self.k):\n",
    "\t\t\tself.frames.append(ob)\n",
    "\t\treturn self._get_ob()\n",
    "\n",
    "\tdef step(self, action):\n",
    "\t\tob, reward, done, info = self.env.step(action)\n",
    "\t\tself.frames.append(ob)\n",
    "\t\treturn self._get_ob(), reward, done, info\n",
    "\n",
    "\tdef _get_ob(self):\n",
    "\t\tassert len(self.frames) == self.k\n",
    "\t\treturn LazyFrames(list(self.frames))\n",
    "\n",
    "class ScaledFloatFrame(gym.ObservationWrapper):\n",
    "\tdef __init__(self, env):\n",
    "\t\tgym.ObservationWrapper.__init__(self, env)\n",
    "\t\tself.observation_space = gym.spaces.Box(low=0, high=1, shape=env.observation_space.shape, dtype=np.float32)\n",
    "\n",
    "\tdef observation(self, observation):\n",
    "\t\t# careful! This undoes the memory optimization, use\n",
    "\t\t# with smaller replay buffers only.\n",
    "\t\treturn np.array(observation).astype(np.float32) / 255.0\n",
    "\n",
    "class LazyFrames(object):\n",
    "\tdef __init__(self, frames):\n",
    "\t\t\"\"\"This object ensures that common frames between the observations are only stored once.\n",
    "\t\tIt exists purely to optimize memory usage which can be huge for DQN's 1M frames replay\n",
    "\t\tbuffers.\n",
    "\t\tThis object should only be converted to numpy array before being passed to the model.\n",
    "\t\tYou'd not believe how complex the previous solution was.\"\"\"\n",
    "\t\tself._frames = frames\n",
    "\t\tself._out = None\n",
    "\n",
    "\tdef _force(self):\n",
    "\t\tif self._out is None:\n",
    "\t\t\tself._out = np.concatenate(self._frames, axis=-1)\n",
    "\t\t\tself._frames = None\n",
    "\t\treturn self._out\n",
    "\n",
    "\tdef __array__(self, dtype=None):\n",
    "\t\tout = self._force()\n",
    "\t\tif dtype is not None:\n",
    "\t\t\tout = out.astype(dtype)\n",
    "\t\treturn out\n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\treturn len(self._force())\n",
    "\n",
    "\tdef __getitem__(self, i):\n",
    "\t\treturn self._force()[i]\n",
    "\n",
    "\tdef count(self):\n",
    "\t\tframes = self._force()\n",
    "\t\treturn frames.shape[frames.ndim - 1]\n",
    "\n",
    "\tdef frame(self, i):\n",
    "\t\treturn self._force()[..., i]\n",
    "\n",
    "def make_atari(env_id, max_episode_steps=None):\n",
    "\tenv = gym.make(env_id)\n",
    "\tassert 'NoFrameskip' in env.spec.id\n",
    "\tenv = NoopResetEnv(env, noop_max=30)\n",
    "\tenv = MaxAndSkipEnv(env, skip=4)\n",
    "\tif max_episode_steps is not None:\n",
    "\t\tenv = TimeLimit(env, max_episode_steps=max_episode_steps)\n",
    "\treturn env\n",
    "\n",
    "def wrap_deepmind(env, episodic_life=True, clip_rewards=True, frame_stack=True, scale=False):\n",
    "\t\"\"\"Configure environment for DeepMind-style Atari.\n",
    "\t\"\"\"\n",
    "\tif episodic_life:\n",
    "\t\tenv = EpisodicLifeEnv(env)\n",
    "\tif 'FIRE' in env.unwrapped.get_action_meanings():\n",
    "\t\tenv = FireResetEnv(env)\n",
    "\tenv = WarpFrame(env)\n",
    "\tif scale:\n",
    "\t\tenv = ScaledFloatFrame(env)\n",
    "\tif frame_stack:\n",
    "\t\tenv = FrameStack(env, 4)\n",
    "\tif clip_rewards:\n",
    "\t\tenv = ClipRewardEnv(env)\n",
    "   \n",
    "\treturn env\n",
    "\n",
    "class ImageToPyTorch(gym.ObservationWrapper):\n",
    "\t\"\"\"\n",
    "\tImage shape to num_channels x weight x height\n",
    "\t\"\"\"\n",
    "\tdef __init__(self, env):\n",
    "\t\tsuper(ImageToPyTorch, self).__init__(env)\n",
    "\t\told_shape = self.observation_space.shape\n",
    "\t\tself.observation_space = gym.spaces.Box(low=0.0, high=1.0, shape=(old_shape[-1], old_shape[0], old_shape[1]), dtype=np.uint8)\n",
    "\n",
    "\tdef observation(self, observation):\n",
    "\t\treturn np.swapaxes(observation, 2, 0)\n",
    "\t\n",
    "def wrap_pytorch(env):\n",
    "\treturn ImageToPyTorch(env)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2sCo-oy66hvM"
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "BATCH_SIZE = 64\n",
    "LR = 1e-4\n",
    "INITIAL_EPSILON = 0.9\n",
    "FINAL_EPSILON = 0.95\n",
    "EPSILON_DECAY = 200000\n",
    "GAMMA = 0.9\n",
    "TARGET_UPDATE_INTERVAL = 1000\n",
    "MODEL_SAVE_INTERVAL = 50000\n",
    "TENSORBOARD_LOG = False\n",
    "#TB_LOG_PATH = './runs/dqn/run2'\n",
    "REPLAY_BUFFER_CAPACITY = 30000\n",
    "env = make_atari(\"PongNoFrameskip-v4\")\n",
    "env = wrap_deepmind(env)\n",
    "env = wrap_pytorch(env)\n",
    "STATE_DIM = env.observation_space.shape[0]\n",
    "ACTION_DIM = env.action_space.n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Cn601ypu6ka7"
   },
   "outputs": [],
   "source": [
    "# network definition\n",
    "class Flatten(torch.nn.Module):\n",
    "\tdef forward(self, x):\n",
    "\t\tbatch_size = x.shape[0]\n",
    "\t\treturn x.view(batch_size, -1)       \n",
    "\n",
    "class ConvDQN(nn.Module):\n",
    "\tdef __init__(self):\n",
    "\t\tsuper(ConvDQN, self).__init__()\n",
    "\n",
    "\t\tself.c1 = nn.Conv2d(STATE_DIM , 32, kernel_size = 8, stride = 4)\n",
    "\t\tself.c2 = nn.Conv2d(32 , 64, kernel_size = 4, stride = 2)\n",
    "\t\tself.c3 = nn.Conv2d(64 , 64, kernel_size = 3, stride = 1)\n",
    "\t\tself.fc1 = nn.Linear(7*7*64, 512)\n",
    "\t\tself.fc2 = nn.Linear(512, ACTION_DIM)\n",
    "\t\n",
    "\t\tself.apply(init_weights)\n",
    "\n",
    "\t\t\n",
    "\tdef forward(self,x):\n",
    "\t\tx = F.relu(self.c1(x))\n",
    "\t\tx = F.relu(self.c2(x))\n",
    "\t\tx = F.relu(self.c3(x))\n",
    "\t\tx = Flatten().forward(x)\n",
    "\t\tx = F.relu(self.fc1(x))\n",
    "\t\tx = self.fc2(x)\n",
    "\t\t\n",
    "\t\treturn x\n",
    "  \n",
    "class DQN(nn.Module):\n",
    "\tdef __init__(self):\n",
    "\t\tsuper(DQN,self).__init__()\n",
    "\n",
    "\t\tself.fc1 = nn.Linear(STATE_DIM,50)\n",
    "\t\tself.fc2 = nn.Linear(50,ACTION_DIM)\n",
    "\n",
    "\t\tself.apply(init_weights)\n",
    "\n",
    "\tdef forward(self,x):\n",
    "\t\tx = F.relu(self.fc1(x))\n",
    "\t\tx = self.fc2(x)\n",
    "\n",
    "\t\treturn x\n",
    "\n",
    "class Agent(object):\n",
    "\tdef __init__(self):\n",
    "\t\tself.dqn, self.target_dqn = ConvDQN(), ConvDQN()\n",
    "\n",
    "\t\tself.learn_step_counter = 0\n",
    "\t\tself.memory_counter = 0\n",
    "\t\tself.replay_buffer = ReplayBuffer(REPLAY_BUFFER_CAPACITY)\n",
    "\t\tself.optimizer = opt.Adam(self.dqn.parameters(),lr=LR)\n",
    "\t\tself.loss_fn = nn.MSELoss()\n",
    "\t\tself.EPSILON = INITIAL_EPSILON\n",
    "\t\n",
    "\t\t\n",
    "\n",
    "\tdef get_action(self, s):\n",
    "\t\ts = torch.unsqueeze(torch.FloatTensor(s),0)\n",
    "\t\n",
    "\n",
    "\t\tif self.EPSILON < FINAL_EPSILON:\n",
    "\t\t\tself.EPSILON += (FINAL_EPSILON - INITIAL_EPSILON)/EPSILON_DECAY\n",
    "\t\n",
    "\n",
    "\t\tif np.random.uniform() < self.EPSILON:\n",
    "\t\t\tqs = self.dqn.forward(s)\n",
    "\t\t\taction = torch.max(qs,1)[1].data.numpy()\n",
    "\t\t\taction = action[0]\n",
    "\t\telse:\n",
    "\t\t\taction = env.action_space.sample()\n",
    "\n",
    "\t\treturn action\n",
    "\n",
    "\tdef update_params(self):\n",
    "\t\t# update target network\n",
    "\t\tif self.learn_step_counter % TARGET_UPDATE_INTERVAL == 0:\n",
    "\t\t\tself.target_dqn.load_state_dict(self.dqn.state_dict())\n",
    "\t\tif self.learn_step_counter % MODEL_SAVE_INTERVAL == 0:\n",
    "\t\t\ttorch.save(self.target_dqn.state_dict(), './pong{}.pth'.format(int(self.learn_step_counter/TARGET_UPDATE_INTERVAL)))\n",
    "\t\tself.learn_step_counter += 1\n",
    "\n",
    "\t\t# sample batch of transitions\n",
    "\t\tstates, actions, rewards, next_states, dones = self.replay_buffer.sample(BATCH_SIZE)\n",
    "\n",
    "\t\tstates = torch.FloatTensor(states)\n",
    "\t\tactions = torch.LongTensor(actions.astype(int).reshape((-1,1)))\n",
    "\t\trewards = torch.FloatTensor(rewards).unsqueeze(1)\n",
    "\t\tnext_states = torch.FloatTensor(next_states)\n",
    "\t\tdones = torch.FloatTensor(np.float32(dones)).unsqueeze(1)\n",
    "\n",
    "\t\t# get q values\n",
    "\t\tq_current = self.dqn(states).gather(1,actions)\n",
    "\t\tindex = torch.max(self.dqn(next_states), 1)[1]\n",
    "\t\tindex = index.view(index.size(0), 1)\n",
    "\t\tq_next = self.target_dqn(next_states).detach()\n",
    "\t\tmax_q_next = q_next.gather(1, index)\n",
    "\t\tq_target = rewards + GAMMA * max_q_next\n",
    "\t\tq_loss = self.loss_fn(q_current,q_target)\n",
    "\t\t\n",
    "\t\t# backpropagate\n",
    "\t\tself.optimizer.zero_grad()\n",
    "\t\tq_loss.backward()\n",
    "\t\tself.optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "MS75o0GC6ohg",
    "outputId": "665e5be9-1494-44f8-c648-9fbe2acdaf69"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on Tesla P100-PCIE-16GB\n",
      "\n",
      "Collecting experience\n",
      "\n",
      "Episode: 0, Frames: 759, Reward: -21.0, Time: 1.82s\n",
      "Episode: 1, Frames: 1598, Reward: -20.0, Time: 1.9s\n",
      "Episode: 2, Frames: 2483, Reward: -21.0, Time: 2.01s\n",
      "Episode: 3, Frames: 3345, Reward: -20.0, Time: 1.99s\n",
      "Episode: 4, Frames: 4369, Reward: -19.0, Time: 2.3s\n",
      "Episode: 5, Frames: 5206, Reward: -20.0, Time: 1.89s\n",
      "Episode: 6, Frames: 6045, Reward: -20.0, Time: 1.91s\n",
      "Episode: 7, Frames: 6829, Reward: -21.0, Time: 1.82s\n",
      "Episode: 8, Frames: 7697, Reward: -20.0, Time: 1.95s\n",
      "Episode: 9, Frames: 8453, Reward: -21.0, Time: 1.67s\n",
      "Episode: 10, Frames: 9210, Reward: -21.0, Time: 1.73s\n",
      "Episode: 11, Frames: 10027, Reward: -21.0, Time: 1.85s\n",
      "Episode: 12, Frames: 10843, Reward: -21.0, Time: 1.78s\n",
      "Episode: 13, Frames: 11659, Reward: -21.0, Time: 1.8s\n",
      "Episode: 14, Frames: 12415, Reward: -21.0, Time: 1.66s\n",
      "Episode: 15, Frames: 13261, Reward: -21.0, Time: 1.87s\n",
      "Episode: 16, Frames: 14180, Reward: -19.0, Time: 2.03s\n",
      "Episode: 17, Frames: 15074, Reward: -20.0, Time: 1.95s\n",
      "Episode: 18, Frames: 15918, Reward: -21.0, Time: 1.87s\n",
      "Episode: 19, Frames: 16754, Reward: -20.0, Time: 1.86s\n",
      "Episode: 20, Frames: 17514, Reward: -21.0, Time: 1.68s\n",
      "Episode: 21, Frames: 18300, Reward: -21.0, Time: 1.73s\n",
      "Episode: 22, Frames: 19089, Reward: -21.0, Time: 1.73s\n",
      "Episode: 23, Frames: 19879, Reward: -21.0, Time: 1.75s\n",
      "Episode: 24, Frames: 20669, Reward: -21.0, Time: 1.72s\n",
      "Episode: 25, Frames: 21510, Reward: -21.0, Time: 1.91s\n",
      "Episode: 26, Frames: 22271, Reward: -21.0, Time: 1.7s\n",
      "Episode: 27, Frames: 23033, Reward: -21.0, Time: 1.71s\n",
      "Episode: 28, Frames: 23788, Reward: -21.0, Time: 1.72s\n",
      "Episode: 29, Frames: 24544, Reward: -21.0, Time: 1.72s\n",
      "Episode: 30, Frames: 25303, Reward: -21.0, Time: 1.68s\n",
      "Episode: 31, Frames: 26059, Reward: -21.0, Time: 1.7s\n",
      "Episode: 32, Frames: 26848, Reward: -21.0, Time: 1.77s\n",
      "Episode: 33, Frames: 27637, Reward: -21.0, Time: 1.79s\n",
      "Episode: 34, Frames: 28456, Reward: -21.0, Time: 1.89s\n",
      "Episode: 35, Frames: 29211, Reward: -21.0, Time: 1.74s\n",
      "\n",
      "Experience Collected !\n",
      "\n",
      "Episode: 36, Frames: 30026, Reward: -21.0, Time: 5.6s\n",
      "Episode: 37, Frames: 30812, Reward: -21.0, Time: 109.52s\n",
      "Episode: 38, Frames: 31601, Reward: -21.0, Time: 114.6s\n",
      "Episode: 39, Frames: 32436, Reward: -20.0, Time: 118.14s\n",
      "Episode: 40, Frames: 33270, Reward: -20.0, Time: 123.21s\n",
      "Episode: 41, Frames: 34030, Reward: -21.0, Time: 114.3s\n",
      "Episode: 42, Frames: 34867, Reward: -20.0, Time: 126.03s\n",
      "Episode: 43, Frames: 35624, Reward: -21.0, Time: 114.89s\n",
      "Episode: 44, Frames: 36445, Reward: -21.0, Time: 121.63s\n",
      "Episode: 45, Frames: 37278, Reward: -20.0, Time: 126.99s\n",
      "Episode: 46, Frames: 38080, Reward: -21.0, Time: 128.16s\n",
      "Episode: 47, Frames: 38837, Reward: -21.0, Time: 117.4s\n",
      "Episode: 48, Frames: 39669, Reward: -20.0, Time: 129.09s\n",
      "Episode: 49, Frames: 40454, Reward: -21.0, Time: 117.49s\n",
      "Episode: 50, Frames: 41468, Reward: -20.0, Time: 152.48s\n",
      "Episode: 51, Frames: 42394, Reward: -19.0, Time: 138.86s\n",
      "Episode: 52, Frames: 43149, Reward: -21.0, Time: 111.17s\n",
      "Episode: 53, Frames: 44028, Reward: -21.0, Time: 130.52s\n",
      "Episode: 54, Frames: 44845, Reward: -21.0, Time: 120.99s\n",
      "Episode: 55, Frames: 45771, Reward: -20.0, Time: 137.1s\n",
      "Episode: 56, Frames: 46699, Reward: -21.0, Time: 135.62s\n",
      "Episode: 57, Frames: 47643, Reward: -20.0, Time: 137.52s\n",
      "Episode: 58, Frames: 48494, Reward: -21.0, Time: 123.66s\n",
      "Episode: 59, Frames: 49383, Reward: -20.0, Time: 128.66s\n",
      "Episode: 60, Frames: 50143, Reward: -21.0, Time: 110.94s\n",
      "Episode: 61, Frames: 50904, Reward: -21.0, Time: 111.33s\n",
      "Episode: 62, Frames: 51693, Reward: -21.0, Time: 114.85s\n",
      "Episode: 63, Frames: 52450, Reward: -21.0, Time: 110.76s\n",
      "Episode: 64, Frames: 53389, Reward: -20.0, Time: 137.65s\n",
      "Episode: 65, Frames: 54173, Reward: -21.0, Time: 115.67s\n",
      "Episode: 66, Frames: 55071, Reward: -20.0, Time: 132.89s\n",
      "Episode: 67, Frames: 56040, Reward: -20.0, Time: 143.41s\n",
      "Episode: 68, Frames: 56930, Reward: -21.0, Time: 133.04s\n",
      "Episode: 69, Frames: 57804, Reward: -20.0, Time: 130.41s\n",
      "Episode: 70, Frames: 58671, Reward: -21.0, Time: 129.66s\n",
      "Episode: 71, Frames: 59658, Reward: -21.0, Time: 147.23s\n",
      "Episode: 72, Frames: 60442, Reward: -21.0, Time: 117.22s\n",
      "Episode: 73, Frames: 61483, Reward: -20.0, Time: 155.58s\n",
      "Episode: 74, Frames: 62574, Reward: -19.0, Time: 162.16s\n",
      "Episode: 75, Frames: 63426, Reward: -21.0, Time: 126.61s\n",
      "Episode: 76, Frames: 64517, Reward: -18.0, Time: 162.46s\n",
      "Episode: 77, Frames: 65448, Reward: -20.0, Time: 139.08s\n",
      "Episode: 78, Frames: 66644, Reward: -19.0, Time: 178.42s\n",
      "Episode: 79, Frames: 67600, Reward: -21.0, Time: 142.22s\n",
      "Episode: 80, Frames: 68754, Reward: -19.0, Time: 172.16s\n",
      "Episode: 81, Frames: 69613, Reward: -21.0, Time: 128.03s\n",
      "Episode: 82, Frames: 70657, Reward: -20.0, Time: 156.03s\n",
      "Episode: 83, Frames: 71719, Reward: -20.0, Time: 158.89s\n",
      "Episode: 84, Frames: 72617, Reward: -20.0, Time: 134.31s\n",
      "Episode: 85, Frames: 73960, Reward: -17.0, Time: 198.79s\n",
      "Episode: 86, Frames: 75272, Reward: -19.0, Time: 191.89s\n",
      "Episode: 87, Frames: 76229, Reward: -19.0, Time: 143.83s\n",
      "Episode: 88, Frames: 77469, Reward: -18.0, Time: 187.03s\n",
      "Episode: 89, Frames: 78825, Reward: -19.0, Time: 202.86s\n",
      "Episode: 90, Frames: 80201, Reward: -18.0, Time: 204.62s\n",
      "Episode: 91, Frames: 81294, Reward: -19.0, Time: 161.47s\n",
      "Episode: 92, Frames: 82397, Reward: -20.0, Time: 162.13s\n",
      "Episode: 93, Frames: 83500, Reward: -19.0, Time: 162.17s\n",
      "Episode: 94, Frames: 84959, Reward: -17.0, Time: 211.91s\n",
      "Episode: 95, Frames: 85973, Reward: -21.0, Time: 146.32s\n",
      "Episode: 96, Frames: 87037, Reward: -20.0, Time: 152.11s\n",
      "Episode: 97, Frames: 88474, Reward: -17.0, Time: 204.83s\n",
      "Episode: 98, Frames: 89870, Reward: -19.0, Time: 196.54s\n",
      "Episode: 99, Frames: 91203, Reward: -16.0, Time: 187.34s\n",
      "Episode: 100, Frames: 92487, Reward: -17.0, Time: 181.03s\n",
      "Episode: 101, Frames: 94028, Reward: -18.0, Time: 216.0s\n",
      "Episode: 102, Frames: 95874, Reward: -18.0, Time: 255.33s\n",
      "Episode: 103, Frames: 97422, Reward: -19.0, Time: 213.31s\n",
      "Episode: 104, Frames: 99704, Reward: -9.0, Time: 312.84s\n",
      "Episode: 105, Frames: 101630, Reward: -16.0, Time: 262.37s\n",
      "Episode: 106, Frames: 103184, Reward: -16.0, Time: 210.44s\n",
      "Episode: 107, Frames: 104541, Reward: -16.0, Time: 182.99s\n",
      "Episode: 108, Frames: 106153, Reward: -19.0, Time: 217.58s\n",
      "Episode: 109, Frames: 107632, Reward: -19.0, Time: 198.31s\n",
      "Episode: 110, Frames: 109204, Reward: -17.0, Time: 210.45s\n",
      "Episode: 111, Frames: 111137, Reward: -14.0, Time: 258.49s\n",
      "Episode: 112, Frames: 112466, Reward: -18.0, Time: 176.81s\n",
      "Episode: 113, Frames: 114159, Reward: -17.0, Time: 225.29s\n",
      "Episode: 114, Frames: 115718, Reward: -20.0, Time: 206.69s\n",
      "Episode: 115, Frames: 117551, Reward: -16.0, Time: 243.27s\n",
      "Episode: 116, Frames: 118818, Reward: -20.0, Time: 169.22s\n",
      "Episode: 117, Frames: 120424, Reward: -16.0, Time: 213.17s\n",
      "Episode: 118, Frames: 122050, Reward: -19.0, Time: 215.71s\n",
      "Episode: 119, Frames: 123641, Reward: -16.0, Time: 210.62s\n",
      "Episode: 120, Frames: 125135, Reward: -17.0, Time: 198.68s\n",
      "Episode: 121, Frames: 126569, Reward: -21.0, Time: 190.19s\n",
      "Episode: 122, Frames: 128339, Reward: -16.0, Time: 234.95s\n",
      "Episode: 123, Frames: 129896, Reward: -19.0, Time: 206.45s\n",
      "Episode: 124, Frames: 131731, Reward: -15.0, Time: 243.81s\n",
      "Episode: 125, Frames: 133636, Reward: -17.0, Time: 253.41s\n",
      "Episode: 126, Frames: 135380, Reward: -18.0, Time: 231.23s\n",
      "Episode: 127, Frames: 137107, Reward: -18.0, Time: 230.21s\n",
      "Episode: 128, Frames: 139153, Reward: -16.0, Time: 271.69s\n"
     ]
    }
   ],
   "source": [
    "# create agent\n",
    "agent = Agent()\n",
    "if TENSORBOARD_LOG:\n",
    "   writer = SummaryWriter(TB_LOG_PATH)\n",
    "\n",
    "print(\"Running on\", torch.cuda.get_device_name(device = torch.cuda.current_device()))\n",
    "print('\\nCollecting experience\\n')\n",
    "for ep in range(2000):\n",
    "\tstate = env.reset()\n",
    "\tepisode_reward = 0\n",
    "\tstart = time.time()\n",
    "\n",
    "\twhile True:\n",
    "\t\t#env.render()\n",
    "\t\taction = agent.get_action(state)\n",
    "\n",
    "\t\t# take action\n",
    "\t\tnext_state, reward_orig, done, _ = env.step(action)\n",
    "\n",
    "\t\tagent.replay_buffer.push((state,action,reward_orig,next_state,done))\n",
    "\t\tagent.memory_counter += 1\n",
    "\n",
    "\t\tepisode_reward += reward_orig\n",
    "\n",
    "\t\tif agent.memory_counter > REPLAY_BUFFER_CAPACITY:\n",
    "\t\t\tagent.update_params()\n",
    "\t\tif agent.memory_counter == REPLAY_BUFFER_CAPACITY:\n",
    "   \t\t\tprint(\"\\nExperience Collected !\\n\")\n",
    "\n",
    "\t\tif done:\n",
    "\t\t\tend = time.time()\n",
    "\t\t\tprint(\"Episode: {}, Frames: {}, Reward: {}, Time: {}s\".format(ep,agent.memory_counter,round(episode_reward,2),round(end-start,2)))\n",
    "\t\t\n",
    "\t\t\tbreak\n",
    "\n",
    "\t\tstate = next_state\n",
    "\tif TENSORBOARD_LOG:\n",
    "\t\twriter.add_scalar('episode_reward',episode_reward,ep)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FzIljWVABuTc"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This is the testing loop that can be run after training. An agent will be initialized on \n",
    "the basis of the latest weights acquired by the trained agents target network and it will play \n",
    "for desired number of episodes\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "bot = agent.target_dqn\n",
    "bot.eval()\n",
    "env = make_atari(\"PongNoFrameskip-v4\")\n",
    "env = wrap_deepmind(env)\n",
    "env = wrap_pytorch(env)\n",
    "env = gym.wrappers.Monitor(env, './videos/Pong', force = True) #You can set the path where your testing videos will be saved\n",
    "for ep in range(2): #The range of the loop is the number of episodes it will test for\n",
    "\tstate = env.reset()\n",
    "\tepisode_reward = 0\n",
    "\tstart = time.time()\n",
    "\n",
    "\twhile True:\n",
    "\t\t#env.render()\n",
    "\t\tstate = torch.unsqueeze(torch.FloatTensor(state),0)\n",
    "\t\tqs = bot.forward(state)\n",
    "\t\taction = torch.max(qs,1)[1].data.numpy()\n",
    "\t\taction = action[0]\n",
    "\n",
    "\t\t# take action\n",
    "\t\tnext_state, reward_orig, done, _ = env.step(action)\n",
    "\n",
    "\t\tepisode_reward += reward_orig\n",
    "\t\n",
    "\t\tif done:\n",
    "\t\t\tend = time.time()\n",
    "\t\t\tprint(\"Episode: {}, Reward: {}, Time: {}s\".format(ep,round(episode_reward,2),round(end-start,2)))\n",
    "\t\t\n",
    "\t\t\tbreak\n",
    "\n",
    "\t\tstate = next_state"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Pong.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
